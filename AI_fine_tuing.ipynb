{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e7b0017-9ec6-4f22-a5d8-401c446eceb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0.dev20250515+cu128\n",
      "NVIDIA GeForce RTX 5070 Ti\n",
      "CUDA 是否可用： True\n",
      "PyTorch 使用的 CUDA 版本： 12.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(\"CUDA 是否可用：\", torch.cuda.is_available())\n",
    "print(\"PyTorch 使用的 CUDA 版本：\", torch.version.cuda)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "745bdd32-3efa-40ad-85ce-c9dd18f75cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e6e70b6d2c4184a3d895a89e5302eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=0, description='Test Slider')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "slider = widgets.IntSlider(description='Test Slider')\n",
    "display(slider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fcfdf3-525c-494d-a5c7-41a79e262c52",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "716b1ba4a15a41e480fea8baa394774a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.1243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成的文字： Hello, world! We’re the new folks.\n",
      "This is our first post. We’re both new to the blogosphere.\n",
      "Hopefully, we can post frequently, and be a bit more interactive than some blogs. We’ll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cafe\\AppData\\Local\\Temp\\ipykernel_12356\\1219571844.py:103: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:879: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 29:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.401500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.408000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.223000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.272600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.250200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.244100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.210300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.274900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.243200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.569100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.677400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.525100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.326500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.404600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.346500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.387300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.532700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.377200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.386800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.217900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.298100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.278300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.324800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.216600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.222700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.328800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.270600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.572100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.512300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.116300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.446200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>2.564500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>6.211500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>7.250300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>8.379000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>9.554400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>8.078900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>7.342700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>8.862100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.722700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>1.846100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.506000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.494400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.448800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.573700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.720100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>1.902200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.845200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>1.399700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.336600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>2.378300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>3.511900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>4.566400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>4.538900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>5.858600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>8.234500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>8.094900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>9.466600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>9.496500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>9.471300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>10.304300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>10.125600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>9.838100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>9.171800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>8.652600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>8.604400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>8.894200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>7.926800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>6.236900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>4.219100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>2.424100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1.785900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>1.655800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>1.753300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.471800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>1.276100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>1.305800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>1.496800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>1.353800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.528200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>1.657900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>1.612300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>1.617000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>1.451000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.450300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>1.662700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>1.488100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>1.496600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>1.579200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.598600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>1.598400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>1.562800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>1.502500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>1.450300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.394100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>1.803500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>1.612300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>1.675300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>1.552600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.747400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>2.026900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>1.650100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>1.839300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>2.082200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>2.239000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>2.451100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>2.186200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>2.389500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>2.549700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.610600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>2.276600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>2.277800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>2.308600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>2.709700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>2.519200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>2.467700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>2.130200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>2.414400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>2.501400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.456800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>2.689300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>2.356500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>2.488300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>2.338800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>2.502900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>2.555800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>2.785600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>2.838500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>2.287400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.326700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>2.709600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>2.379000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>2.477800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>2.487200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>2.498700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>2.167700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>2.368600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>2.298900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>2.297500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.445400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>2.285600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>2.387400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>2.194700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>2.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>2.457300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>2.241800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>2.273300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>2.518600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>2.580300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.254900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6829e55c-0ffab87014b573a344b3a34c;0d01e35b-6b66-4d18-bd64-15a5aa3bf766)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:879: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6829e5d1-1bef6cfb0ac99ace2561effc;975f6a1b-63e5-4362-854a-14290bb5e3ac)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:879: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6829e649-7e55aac16ef78fc84de58fee;0934e8d7-7ddf-473b-835c-c21855b0f198)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:879: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6829e6bf-2528ec651f769b3a4c876aa9;0684674e-6459-4eab-9ce9-edb9dd177818)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:879: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6829e73a-5b3a9af472ad5dbf7f760024;48e82da6-c082-431e-b9ed-6167935a5925)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:879: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6829e7b2-005a3d5e30c80e936212b5a3;9e9266b4-9b8b-4ef7-afd2-9d265452f10a)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:879: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6829e828-3156721150c8c45627f33eee;672cdb91-91d3-4191-82c4-c9bf08a07c1e)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:879: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6829e8a1-59efc0512490f43d3612b415;b15048c0-9331-4c51-8b76-7dee246d006f)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:879: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6829e91b-01d9db67143176751c9b12eb;b39c1b57-6281-4a80-978b-2f39894d2293)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:879: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6829e995-615140ca592404832b373c5f;f9dc1561-8141-482a-a9b0-33f2d89529ae)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:879: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6829ea0c-3186c59c1248f4d40febdfed;f52e094a-e531-47ba-8951-e440fd768981)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:879: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6829ea87-652a6e2e0d86dd2a10f9d2e1;7dee3049-56da-46de-a9ab-64933c56a165)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:879: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6829eaff-24a34bc879b9f0a534b53a07;775b19e1-547c-4e34-a356-e685dbf2f573)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:879: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6829eb76-067cda137e525e7c2267d6db;e1872f8c-18fc-44d4-8baa-3e5e022afd68)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:879: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6829ebec-241ab26447c8f72a1dd84e2e;5bf8d466-6f57-4156-94a4-547bc92d5bea)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1500, training_loss=2.9113416430155437, metrics={'train_runtime': 1798.0956, 'train_samples_per_second': 1.668, 'train_steps_per_second': 0.834, 'total_flos': 6.0970588176384e+16, 'train_loss': 2.9113416430155437, 'epoch': 3.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# 模型與 tokenizer\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "access_token = \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=access_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # ✅ 這行是關鍵\n",
    "\n",
    "# 量化設定（8bit）\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "# 載入模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    token=access_token,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# 轉換為 LoRA 可訓練模型\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA 設定\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# 測試生成文字\n",
    "input_text = \"Hello, world!\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        temperature=0.8\n",
    "    )\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"生成的文字：\", output_text)\n",
    "\n",
    "# 載入與處理資料集\n",
    "dataset = load_dataset(\"mlabonne/guanaco-llama2-1k\", split=\"train\")\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# 訓練參數\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    fp16=False,  # 如果你用的是支援 fp16 的 GPU，可設為 True\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"tensorboard\",\n",
    "    remove_unused_columns=False  # ✅ 加上這個避免 text 欄位被自動移除\n",
    ")\n",
    "\n",
    "# Trainer 設定\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20140480-f191-4a42-855d-bc61fcbf8473",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6829ed11-453f87f819ec60590b89b609;ebb91fef-4761-43c1-8010-eed7f132548e)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a8b13da315447a9984e7485f1133c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2425: UserWarning: for base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2425: UserWarning: for base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2425: UserWarning: for base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2425: UserWarning: for base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2425: UserWarning: for base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2425: UserWarning: for base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2425: UserWarning: for base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2425: UserWarning: for base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2425: UserWarning: for base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2425: UserWarning: for base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2425: UserWarning: for base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2425: UserWarning: for base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2425: UserWarning: for base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2425: UserWarning: for base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2425: UserWarning: for base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2425: UserWarning: for base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2425: UserWarning: for base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2425: UserWarning: for base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2425: UserWarning: for base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2425: UserWarning: for base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'base_model.model.model.lm_head'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m base_model = AutoModelForCausalLM.from_pretrained(model_id, token=access_token, device_map=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# 2. 加載 LoRA adapter\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m model = PeftModel.from_pretrained(base_model, \u001b[33m\"\u001b[39m\u001b[33mlora_guanaco\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# 3. 合併\u001b[39;00m\n\u001b[32m     15\u001b[39m merged_model = model.merge_and_unload()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\peft\\peft_model.py:541\u001b[39m, in \u001b[36mPeftModel.from_pretrained\u001b[39m\u001b[34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, **kwargs)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    533\u001b[39m     model = MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config.task_type](\n\u001b[32m    534\u001b[39m         model,\n\u001b[32m    535\u001b[39m         config,\n\u001b[32m   (...)\u001b[39m\u001b[32m    538\u001b[39m         low_cpu_mem_usage=low_cpu_mem_usage,\n\u001b[32m    539\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m541\u001b[39m load_result = model.load_adapter(\n\u001b[32m    542\u001b[39m     model_id,\n\u001b[32m    543\u001b[39m     adapter_name,\n\u001b[32m    544\u001b[39m     is_trainable=is_trainable,\n\u001b[32m    545\u001b[39m     autocast_adapter_dtype=autocast_adapter_dtype,\n\u001b[32m    546\u001b[39m     low_cpu_mem_usage=low_cpu_mem_usage,\n\u001b[32m    547\u001b[39m     **kwargs,\n\u001b[32m    548\u001b[39m )\n\u001b[32m    550\u001b[39m \u001b[38;5;66;03m# 1. Remove VB-LoRA vector bank, since it's a shared parameter set via the VBLoRAModel\u001b[39;00m\n\u001b[32m    551\u001b[39m \u001b[38;5;66;03m# 2. Remove the prompt encoder, as it does not need to be part of the checkpoint\u001b[39;00m\n\u001b[32m    552\u001b[39m missing_keys = [\n\u001b[32m    553\u001b[39m     k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m load_result.missing_keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mvblora_vector_bank\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m k \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mprompt_encoder\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m k\n\u001b[32m    554\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\peft\\peft_model.py:1327\u001b[39m, in \u001b[36mPeftModel.load_adapter\u001b[39m\u001b[34m(self, model_id, adapter_name, is_trainable, torch_device, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, **kwargs)\u001b[39m\n\u001b[32m   1322\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   1323\u001b[39m     device_map = infer_auto_device_map(\n\u001b[32m   1324\u001b[39m         \u001b[38;5;28mself\u001b[39m, max_memory=max_memory, no_split_module_classes=no_split_module_classes\n\u001b[32m   1325\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1327\u001b[39m \u001b[38;5;28mself\u001b[39m._update_offload(offload_index, adapters_weights)\n\u001b[32m   1328\u001b[39m dispatch_model_kwargs[\u001b[33m\"\u001b[39m\u001b[33moffload_index\u001b[39m\u001b[33m\"\u001b[39m] = offload_index\n\u001b[32m   1330\u001b[39m dispatch_model(\n\u001b[32m   1331\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1332\u001b[39m     device_map=device_map,\n\u001b[32m   1333\u001b[39m     offload_dir=offload_dir,\n\u001b[32m   1334\u001b[39m     **dispatch_model_kwargs,\n\u001b[32m   1335\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\peft\\peft_model.py:1151\u001b[39m, in \u001b[36mPeftModel._update_offload\u001b[39m\u001b[34m(self, offload_index, adapters_weights)\u001b[39m\n\u001b[32m   1149\u001b[39m suffix_pos = safe_key.rfind(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1150\u001b[39m extended_prefix = prefix + block_id + safe_key[:suffix_pos]\n\u001b[32m-> \u001b[39m\u001b[32m1151\u001b[39m safe_module = \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mself\u001b[39m.named_modules())[extended_prefix]\n\u001b[32m   1152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(safe_module, BaseTunerLayer):\n\u001b[32m   1153\u001b[39m     final_key = extended_prefix + \u001b[33m\"\u001b[39m\u001b[33m.base_layer\u001b[39m\u001b[33m\"\u001b[39m + safe_key[suffix_pos:]\n",
      "\u001b[31mKeyError\u001b[39m: 'base_model.model.model.lm_head'"
     ]
    }
   ],
   "source": [
    "# 儲存 LoRA adapter 權重\n",
    "model.save_pretrained(\"lora_guanaco\")\n",
    "tokenizer.save_pretrained(\"lora_guanaco\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3730a299-cbf5-4da6-9290-2f9be2ccd10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d72e9a-f5a3-46ac-95a0-93406ec65cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 開始設定基本參數...\n",
      "⚙️ 設定量化參數...\n",
      "✅ 載入 tokenizer...\n",
      "🧠 載入 base 模型...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f409601f4ed46bd94392ea545f145f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Base 模型載入完成\n",
      "🧠 載入 LoRA 微調模型...\n",
      "✅ LoRA 微調模型載入完成\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import os\n",
    "\n",
    "os.makedirs(\"offload_dir\", exist_ok=True)\n",
    "\n",
    "try:\n",
    "    print(\"🔧 開始設定基本參數...\")\n",
    "    base_model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "    lora_path = \"lora_guanaco\"\n",
    "    access_token = \n",
    "    print(\"⚙️ 設定量化參數...\")\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        llm_int8_enable_fp32_cpu_offload=True\n",
    "    )\n",
    "\n",
    "    print(\"✅ 載入 tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_id, token=access_token, use_fast=False)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    print(\"🧠 載入 base 模型...\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_id,\n",
    "        token=access_token,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "        offload_folder=\"offload_dir\"\n",
    "    )\n",
    "    base_model.eval()\n",
    "    print(\"✅ Base 模型載入完成\")\n",
    "\n",
    "    print(\"🧠 載入 LoRA 微調模型...\")\n",
    "    lora_model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        lora_path,\n",
    "        device_map=\"auto\",\n",
    "        offload_folder=\"offload_dir\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    lora_model.eval()\n",
    "    print(\"✅ LoRA 微調模型載入完成\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"❌ 執行時發生錯誤：\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "854cc51d-25e5-4979-9725-301ca8c4ec54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧾 輸入 prompt: Describe a peaceful sunset by the ocean.\n",
      "🚀 開始用 Base 模型生成文字...\n",
      "✅ Base 模型生成完成\n",
      "📘 [Base 模型 輸出]:\n",
      " I am standing on a beach, surrounded by the serenity of a calm ocean     I am surrounded by the sounds of the\n",
      "🚀 開始用 LoRA 微調模型生成文字...\n",
      "✅ LoRA 模型生成完成\n",
      "📕 [LoRA 微調後模型 輸出]:\n",
      " Sunset by the ocean is a beautiful and tranquil sight, with the sky and water shaded by a soft pink and orange color, and the gentle waves of the ocean crashing against\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_response(model, tokenizer, prompt: str, max_new_tokens=100):\n",
    "    print(\"🧾 輸入 prompt:\", prompt)\n",
    "    \n",
    "    # Tokenize prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.8,\n",
    "            top_p=0.95,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return output_text\n",
    "def clean_generation(text, prompt):\n",
    "    # 移除 prompt 本身\n",
    "    text = text.replace(prompt, \"\")\n",
    "    \n",
    "    # 移除常見標籤\n",
    "    for tag in [\"[INST]\", \"[/INST]\", \"]\", \"[\"]:\n",
    "        text = text.replace(tag, \"\")\n",
    "    \n",
    "    # 去除多餘空白與換行\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "try:\n",
    "    prompt = \"Describe a peaceful sunset by the ocean.\"\n",
    "    print(\"🧾 輸入 prompt:\", prompt)\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(base_model.device)\n",
    "    print(\"🚀 開始用 Base 模型生成文字...\")\n",
    "    with torch.no_grad():\n",
    "        base_output = base_model.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            temperature=0.8\n",
    "        )\n",
    "    print(\"✅ Base 模型生成完成\")\n",
    "    text = tokenizer.decode(base_output[0], skip_special_tokens=True)\n",
    "    clean_text = clean_generation(text, prompt)\n",
    "    \n",
    "    print(\"📘 [Base 模型 輸出]:\\n\", clean_text)\n",
    "    \n",
    "    \n",
    "    print(\"🚀 開始用 LoRA 微調模型生成文字...\")\n",
    "    with torch.no_grad():\n",
    "        lora_output = lora_model.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            temperature=0.8\n",
    "        )\n",
    "    print(\"✅ LoRA 模型生成完成\")\n",
    "    text = tokenizer.decode(lora_output[0], skip_special_tokens=True)\n",
    "    clean_text = clean_generation(text, prompt)\n",
    "    print(\"📕 [LoRA 微調後模型 輸出]:\\n\", clean_text)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"❌ 執行時發生錯誤：\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1dab05-8b9e-46e1-87c8-3f4de9eb6eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:898: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cafe\\anaconda3\\envs\\Ai_fine_tuning\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d138f181a8b4830aa8525eb71dcafa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "access_token = \n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=access_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=access_token,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    resume_download=True\n",
    ")\n",
    "\n",
    "prompt = \"<s>[INST] <<SYS>>\\nYou are a helpful assistant.\\n<</SYS>>\\nDescribe a peaceful sunset by the ocean in one sentence.[/INST]\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5c5e9fdc-e986-40e1-9bf4-e96e16628807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" A serene sunset over a calm ocean, surrounded by a peaceful sea, a salty b\n"
     ]
    }
   ],
   "source": [
    "def clean_generation(text, prompt):\n",
    "    # 移除 prompt 本身\n",
    "    text = text.replace(prompt, \"\")\n",
    "    \n",
    "    # 移除常見標籤\n",
    "    for tag in [\"[INST]\", \"[/INST]\", \"]\", \"[\"]:\n",
    "        text = text.replace(tag, \"\")\n",
    "    \n",
    "    # 去除多餘空白與換行\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "prompt = \"Describe a peaceful sunset by the ocean in one sentence.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(base_model.device)\n",
    "\n",
    "output = base_model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=30,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    temperature=0.8,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "clean_text = clean_generation(text, prompt)\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a612de26-e991-425f-acf3-ba517c257e0f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "資料集大小: 1000\n",
      "{'text': '<s>[INST] Me gradué hace poco de la carrera de medicina ¿Me podrías aconsejar para conseguir rápidamente un puesto de trabajo? [/INST] Esto vale tanto para médicos como para cualquier otra profesión tras finalizar los estudios aniversarios y mi consejo sería preguntar a cuántas personas haya conocido mejor. En este caso, mi primera opción sería hablar con otros profesionales médicos, echar currículos en hospitales y cualquier centro de salud. En paralelo, trabajaría por mejorar mi marca personal como médico mediante un blog o formas digitales de comunicación como los vídeos. Y, para mejorar las posibilidades de encontrar trabajo, también participaría en congresos y encuentros para conseguir más contactos. Y, además de todo lo anterior, seguiría estudiando para presentarme a las oposiciones y ejercer la medicina en el sector público de mi país. </s>'}\n",
      "{'text': '<s>[INST] Самый великий человек из всех живших на планете? [/INST] Для начала нужно выбрать критерии величия человека. Обычно великим называют человека, который внес большой вклад в общество или сильно выделялся на фоне других в своем деле.\\n\\nНапример, Иосифа Бродского считают великим поэтом, а Иммануила Канта — великим философом. Александр Македонский, известный тем, что собрал в свои владения огромную империю (включавшую Македонию, Грецию, Персию, Египет), в историографии носит имя Александр Великий. Для христиан, скорее всего, самым великим человеком жившим на земле был Иисус Христос, так как он совершил множество благих деяний и совершил подвиг ради человечества. \\n\\nПри этом, когда мы выдвигаем одну личность на роль великого человека, сразу же находится множество людей, не согласных с этим. Того же Иосифа Бродского, хоть он и получил престижную Нобелевскую премию, некоторые люди считают графоманом и посредственным поэтом. \\n\\nВ целом, кого считать великим — это самостоятельный выбор каждого человека, который можно сделать только соотнося его со своими личными ценностями и представлениями о красивом, правильном, хорошем. </s>'}\n",
      "{'text': '<s>[INST] Compose a professional email with the following points:\\n\\nMe chinese cook 10 years\\nGood good cook\\nPeople love my food\\nCan you hire me? [/INST] Thanks for the information. Unfortunately, your initial message does not contain enough details or structure to compose a compelling professional email to a potential employer. Here are some suggestions for how we can improve the message:\\n\\n1) Let\\'s first clarify the type of cooking position you are interested in. Are you looking for a job as a line cook, sous chef, executive chef, or something else? This will help determine how to emphasize relevant experience and skills.\\n\\n2) What specific experience do you have? For example, \"Over 10 years of experience preparing authentic Sichuan and Cantonese cuisine.\" Highlight any relevant skills, like knife skills, cooking techniques, menu development, etc. Mention the types of cuisine you specialize in.\\n\\n3) Have you worked in a professional kitchen before? If so, mention the name and type of restaurant (upscale, casual dining, etc.). Emphasize relevant responsibilities and accomplishments. If not, that\\'s fine too, just be clear that you\\'re looking to transition into a professional kitchen and highlight related skills and passion for cooking.\\n\\n4) Do you have any certifications or training? Note any culinary degrees, certificates, or other credentials. Even informal training can show your dedication to the craft.\\n\\n5) Are you looking for full-time, part-time, temporary, or contract work? Be specific about the type of opportunity you\\'re seeking.\\n\\nWith the answers to these questions, we can craft an email in a standard business letter format. The opening can express your interest in the position and summarize your relevant experience and qualifications. The body can go into more depth on your experience, skills, and goals. The closing can reiterate your interest and mention that you\\'d welcome the opportunity to further discuss the role.\\n\\nDoes this help? Let me know if you would like help crafting the full email. I\\'m happy to assist you in composing a professional and compelling message to potential employers. </s>'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"mlabonne/guanaco-llama2-1k\", split=\"train\")\n",
    "\n",
    "# 查看資料集大小\n",
    "print(f\"資料集大小: {len(dataset)}\")\n",
    "\n",
    "# 查看前幾筆資料\n",
    "for i in range(3):\n",
    "    print(dataset[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "839c28c6-4202-48cc-bb63-3de2189b1230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '[', 'INST', ']', '▁What', '▁is', '▁the', '▁meaning', '▁of', '▁life', '?', '▁[', '/', 'INST', ']']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(\"<s>[INST] What is the meaning of life? [/INST]\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "526b49cc-2bf2-4096-96ac-c3d2f901d431",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): lora.Linear8bitLt(\n",
      "                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
      "              (v_proj): lora.Linear8bitLt(\n",
      "                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
      "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
      "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(lora_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e8dca1-efc3-4ad5-9a81-8f9f6d8e90ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
